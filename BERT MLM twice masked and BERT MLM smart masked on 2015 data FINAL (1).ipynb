{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3fa4ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim import downloader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9897eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /anaconda/envs/py38_default/lib/python3.8/site-packages (4.14.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (1.21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (2021.11.2)\n",
      "Requirement already satisfied: sacremoses in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: requests in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/py38_default/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: click in /anaconda/envs/py38_default/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in /anaconda/envs/py38_default/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /anaconda/envs/py38_default/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/anaconda/envs/py38_default/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d458372e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /anaconda/envs/py38_default/lib/python3.8/site-packages (1.17.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: pandas in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (0.2.1)\n",
      "Requirement already satisfied: dill in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (2021.8.1)\n",
      "Requirement already satisfied: aiohttp in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: multiprocess in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (1.21.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: packaging in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: xxhash in /anaconda/envs/py38_default/lib/python3.8/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/py38_default/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
      "Requirement already satisfied: pyyaml in /anaconda/envs/py38_default/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda/envs/py38_default/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/anaconda/envs/py38_default/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67a4095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d368ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9cac22",
   "metadata": {},
   "source": [
    "# Import 2015 DF\n",
    "\n",
    "We use them because they are lighter and faster to train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "806abe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_train_df = pd.read_csv('//home//student//project//project_final_files//nytcrosswords_balanced_days_train_2015.csv',parse_dates = ['Date'], encoding =\"ISO-8859-1\")\n",
    "nyt_eval_df = pd.read_csv('/home//student//project//project_final_files//nytcrosswords_balanced_days_eval_2015.csv',parse_dates = ['Date'], encoding =\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50cd4826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Word</th>\n",
       "      <th>Clue</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-20</td>\n",
       "      <td>DEW</td>\n",
       "      <td>Wet blanket?</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-21</td>\n",
       "      <td>SHOP</td>\n",
       "      <td>Union workplace</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>AWET</td>\n",
       "      <td>Mad as ___ hen</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-12-08</td>\n",
       "      <td>NARNIA</td>\n",
       "      <td>Fictional land of books and film</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-15</td>\n",
       "      <td>ROKU</td>\n",
       "      <td>Giant in media streaming</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117532</th>\n",
       "      <td>2020-07-15</td>\n",
       "      <td>DELTA</td>\n",
       "      <td>Charlie follower</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117533</th>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>TETRAD</td>\n",
       "      <td>Group of four</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117534</th>\n",
       "      <td>2020-12-22</td>\n",
       "      <td>OLD</td>\n",
       "      <td>Aged</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117535</th>\n",
       "      <td>2017-03-05</td>\n",
       "      <td>ROADSIDEDINER</td>\n",
       "      <td>[Circled letters]-advertised establishment</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117536</th>\n",
       "      <td>2019-11-20</td>\n",
       "      <td>ERA</td>\n",
       "      <td>Baseball stat that's better when it's lower</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117537 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date           Word                                         Clue  \\\n",
       "0      2016-08-20            DEW                                 Wet blanket?   \n",
       "1      2020-10-21           SHOP                              Union workplace   \n",
       "2      2015-01-28           AWET                               Mad as ___ hen   \n",
       "3      2019-12-08         NARNIA             Fictional land of books and film   \n",
       "4      2020-09-15           ROKU                     Giant in media streaming   \n",
       "...           ...            ...                                          ...   \n",
       "117532 2020-07-15          DELTA                             Charlie follower   \n",
       "117533 2017-03-08         TETRAD                                Group of four   \n",
       "117534 2020-12-22            OLD                                         Aged   \n",
       "117535 2017-03-05  ROADSIDEDINER   [Circled letters]-advertised establishment   \n",
       "117536 2019-11-20            ERA  Baseball stat that's better when it's lower   \n",
       "\n",
       "       day_of_week  year  \n",
       "0         Saturday  2016  \n",
       "1        Wednesday  2020  \n",
       "2        Wednesday  2015  \n",
       "3           Sunday  2019  \n",
       "4          Tuesday  2020  \n",
       "...            ...   ...  \n",
       "117532   Wednesday  2020  \n",
       "117533   Wednesday  2017  \n",
       "117534     Tuesday  2020  \n",
       "117535      Sunday  2017  \n",
       "117536   Wednesday  2019  \n",
       "\n",
       "[117537 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt_train_df.drop(columns = ['Unnamed: 0'],inplace = True)\n",
    "nyt_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f12f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Word</th>\n",
       "      <th>Clue</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-09-07</td>\n",
       "      <td>INSPIRE</td>\n",
       "      <td>Motivate</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-10-29</td>\n",
       "      <td>ALONE</td>\n",
       "      <td>Stag</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-08-11</td>\n",
       "      <td>EDGES</td>\n",
       "      <td>Things that spheres lack</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-17</td>\n",
       "      <td>TYKE</td>\n",
       "      <td>Young 'un</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-03-20</td>\n",
       "      <td>EUCLID</td>\n",
       "      <td>Who wrote to Ptolemy I \"There is no royal road...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33843</th>\n",
       "      <td>2018-10-29</td>\n",
       "      <td>TNT</td>\n",
       "      <td>Relative of dynamite</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33844</th>\n",
       "      <td>2017-10-12</td>\n",
       "      <td>ATEUP</td>\n",
       "      <td>Believed with no questions asked</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33845</th>\n",
       "      <td>2016-09-14</td>\n",
       "      <td>UTILITY</td>\n",
       "      <td>Gas or water</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33846</th>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>CRI</td>\n",
       "      <td>Shout, in Chamonix</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33847</th>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>GOLD</td>\n",
       "      <td>Top Olympic medal</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33848 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date     Word                                               Clue  \\\n",
       "0     2015-09-07  INSPIRE                                           Motivate   \n",
       "1     2017-10-29    ALONE                                               Stag   \n",
       "2     2020-08-11    EDGES                           Things that spheres lack   \n",
       "3     2018-12-17     TYKE                                          Young 'un   \n",
       "4     2015-03-20   EUCLID  Who wrote to Ptolemy I \"There is no royal road...   \n",
       "...          ...      ...                                                ...   \n",
       "33843 2018-10-29      TNT                               Relative of dynamite   \n",
       "33844 2017-10-12    ATEUP                   Believed with no questions asked   \n",
       "33845 2016-09-14  UTILITY                                       Gas or water   \n",
       "33846 2020-10-17      CRI                                 Shout, in Chamonix   \n",
       "33847 2020-06-29     GOLD                                  Top Olympic medal   \n",
       "\n",
       "      day_of_week  year  \n",
       "0          Monday  2015  \n",
       "1          Sunday  2017  \n",
       "2         Tuesday  2020  \n",
       "3          Monday  2018  \n",
       "4          Friday  2015  \n",
       "...           ...   ...  \n",
       "33843      Monday  2018  \n",
       "33844    Thursday  2017  \n",
       "33845   Wednesday  2016  \n",
       "33846    Saturday  2020  \n",
       "33847      Monday  2020  \n",
       "\n",
       "[33848 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt_eval_df.drop(columns = ['Unnamed: 0'],inplace = True)\n",
    "nyt_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a0b49e",
   "metadata": {},
   "source": [
    "# Data loader class \n",
    "\n",
    "Its here because it relevant for both models as it is ised in the fine-tuning loop to pass the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c14ff823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossWordsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbea17",
   "metadata": {},
   "source": [
    "# BERT MLM on 2015 data masked twice\n",
    "\n",
    "In this section we will try to solve the crosswords with the basic masking technique , which is masking every answer with double mask in the traning set.\n",
    "this technique is derived from the problem that we cant know from the answer that is given to us that the answer is a single word , two words or three words.\n",
    "This also applies to real life New York Times crosswords , there are only hints and a total count of letters in the answer\n",
    "\n",
    "We will use different auxiliary function in each of the model section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0c02cd",
   "metadata": {},
   "source": [
    "**Some auxiliary function for training and evaluating the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c897c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_riddles_and_answers_mask_position_corrected_masked_once(df):\n",
    "    hint_masked_list = []\n",
    "    hist_answer_list = []\n",
    "    for row in df.itertuples():\n",
    "        if '_' in str(row.Clue):\n",
    "            masked = re.sub('___','[MASK]',row.Clue)\n",
    "            answerd = re.sub('___',str(row.Word),row.Clue)\n",
    "        else: # doesnt have ___ , hence second kind of hint\n",
    "            masked = str(row.Clue) + ' : [MASK]'\n",
    "            answerd = str(row.Clue) + ' : '+str(row.Word)\n",
    "        hint_masked_list.append(masked)\n",
    "        hist_answer_list.append(answerd)\n",
    "    return hint_masked_list,hist_answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f516237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_riddles_and_answers_mask_position_corrected_masked_twice(df):\n",
    "    hint_masked_list_bigram = []\n",
    "    hist_answer_list_bigram = []\n",
    "    answers_list = []\n",
    "    for row in df.itertuples():\n",
    "        if '_' in str(row.Clue):\n",
    "            masked = re.sub('___','[MASK] [MASK]',row.Clue)\n",
    "            answerd = re.sub('___',str(row.Word),row.Clue)\n",
    "        else: # doesnt have ___ , hence second kind of hint\n",
    "            masked = str(row.Clue) + ' : [MASK] [MASK]'\n",
    "            answerd = str(row.Clue) + ' : '+str(row.Word)\n",
    "        hint_masked_list_bigram.append(masked)\n",
    "        hist_answer_list_bigram.append(answerd)\n",
    "        answers_list.append(str(row.Word))\n",
    "    return hint_masked_list_bigram,hist_answer_list_bigram , answers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01d83b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_accuracy_uni_plus_bigram(model,tokenizer,masked_tokenized_unigram, masked_tokenized_bigram , answered_untokenzed_list,top_ks = 10):\n",
    "    cnt_uni = 0\n",
    "    cnt_bi = 0\n",
    "    total = len(answered_untokenzed_list)\n",
    "    loop = tqdm(range(total), leave=True)\n",
    "    for i in loop:\n",
    "        answer = answered_untokenzed_list[i]\n",
    "        output_bi = model(input_ids = masked_tokenized_bigram.input_ids[i].to(model.device).unsqueeze(0),\n",
    "                                                   token_type_ids = masked_tokenized_bigram.token_type_ids[i].to(model.device).unsqueeze(0) ,\n",
    "                                                   attention_mask = masked_tokenized_bigram.attention_mask[i].to(model.device).unsqueeze(0))\n",
    "        masked_id_list = []\n",
    "        for mask_id in range(masked_tokenized_bigram.input_ids[i].shape[0]):\n",
    "            if masked_tokenized_bigram.input_ids[i][mask_id] == 103:\n",
    "#                 masked_id_list.append(mask_id+1)\n",
    "                masked_id_list.append(mask_id)\n",
    "        answer_input_ids = []\n",
    "        for mask_id in masked_id_list:\n",
    "            answer_input_ids.append(output_bi.logits[0][mask_id].topk(top_ks).indices)\n",
    "        if ( len(answer_input_ids) < 2):\n",
    "            continue\n",
    "        predictions = []\n",
    "        for tk in range(np.min([answer_input_ids[0].shape[0],answer_input_ids[1].shape[0],top_ks])):\n",
    "            temp = [answer_input_ids[0][tk].item(),answer_input_ids[1][tk].item()]\n",
    "            predictions.extend(tokenizer.decode(temp).split())\n",
    "        if ( answer.lower() in predictions):\n",
    "            cnt_bi += 1\n",
    "            continue\n",
    "        \n",
    "        # else answer bi not in prediction , try unigram\n",
    "        output_uni = model(input_ids = masked_tokenized_unigram.input_ids[i].to(model.device).unsqueeze(0),\n",
    "                                                   token_type_ids = masked_tokenized_unigram.token_type_ids[i].to(model.device).unsqueeze(0) ,\n",
    "                                                   attention_mask = masked_tokenized_unigram.attention_mask[i].to(model.device).unsqueeze(0))\n",
    "        masked_index = -1\n",
    "        for mask_id in range(masked_tokenized_unigram.input_ids[i].shape[0]):\n",
    "            if masked_tokenized_unigram.input_ids[i][mask_id] == 103:\n",
    "                masked_index = mask_id\n",
    "                break\n",
    "        if masked_index == -1:\n",
    "            continue\n",
    "        predictions = []\n",
    "        predictions = tokenizer.decode(output_uni.logits[0][masked_index].topk(top_ks).indices).split()\n",
    "        if ( answer.lower() in predictions):\n",
    "            cnt_uni += 1\n",
    "    print('Count for masked once was: ',cnt_uni,', Count for masked twice was: ',cnt_bi,' both of them sum up to',cnt_bi+cnt_uni ,' out of a total of ',total)    \n",
    "    print('So this brings us to the accuract for top ',top_ks,' prediction model made at',round((cnt_uni + cnt_bi)/total*100,3),'%.')\n",
    "    return cnt_uni,cnt_bi,total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df9974b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_accuracy_by_days(model,tokenizer,df_to_check,top_ks = 10):\n",
    "    days = ['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday']\n",
    "    results_dict = {}\n",
    "    over_all_once = 0\n",
    "    over_all_twice = 0\n",
    "    over_all_total = 0\n",
    "    for day in days:\n",
    "        once_cnt = 0\n",
    "        twice_cnt = 0\n",
    "        total = 0\n",
    "        masked_by_day_one , answered_by_day_one = create_riddles_and_answers_mask_position_corrected_masked_once(df_to_check[df_to_check['day_of_week'] == day])\n",
    "        day_inputs = tokenized_list_for_model(tokenizer , [masked_by_day_one,answered_by_day_one])\n",
    "        masked_by_day_twice , answered_by_day_twice , day_answer_list = create_riddles_and_answers_mask_position_corrected_masked_twice(df_to_check[df_to_check['day_of_week'] == day])\n",
    "        day_inputs_twice = tokenized_list_for_model(tokenizer ,[masked_by_day_twice , answered_by_day_twice] )\n",
    "        print(day,' The results are as follows: ')\n",
    "        once_cnt , twice_cnt , total = get_topk_accuracy_uni_plus_bigram(model , tokenizer , day_inputs,day_inputs_twice,day_answer_list,top_ks)\n",
    "        results_dict[day] = {'total_enteries' : total , 'masked_once_cnt' : once_cnt , 'masked_twice_cnt' : twice_cnt }\n",
    "        over_all_once += once_cnt\n",
    "        over_all_twice += twice_cnt\n",
    "        over_all_total += total\n",
    "    results_dict['overall'] = {'total_enteries' : over_all_total , 'masked_once_cnt' : over_all_once , 'masked_twice_cnt' : over_all_twice}\n",
    "    print('Total score as follows\\nCount for masked once was: ',over_all_once,', Count for masked twice was: ',over_all_twice,' both of them sum up to',over_all_once+over_all_twice ,' out of a total of ',over_all_total)    \n",
    "    print('So this brings us to the accuract for top ',top_ks,' prediction model made at',round((over_all_once + over_all_twice)/over_all_total*100,3),'%.')\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f065df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_list_for_model(tokenizer , list_to_tokenize,pad_max_len = 64): # pass both lists masked and answered\n",
    "    masked_ds_train = tokenizer(list_to_tokenize[0],padding=True,truncation=True,max_length = pad_max_len,return_tensors=\"pt\")\n",
    "    masked_ds_train['labels'] = tokenizer(list_to_tokenize[1],padding=True,truncation=True,max_length = pad_max_len,return_tensors=\"pt\").input_ids.detach().clone()\n",
    "    return masked_ds_train\n",
    "# masked_ds_train = masked_ds_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e59a30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_days_histogram(df):\n",
    "    days_week_dict = {d:df[df['day_of_week'].apply(lambda x: x == d)].shape[0] for d in set(df['day_of_week'])}\n",
    "    days = ['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday']\n",
    "    total = np.sum([v for v in days_week_dict.values()])\n",
    "    for d in days:\n",
    "        print(d,' Values is ',days_week_dict[d],' Which in proportion to other days is: ',round(days_week_dict[d]/total,3))\n",
    "    plt.figure(figsize = (10,5))\n",
    "    plt.bar([i for i in range(len(days_week_dict.keys()))],days_week_dict.values())\n",
    "    plt.title('Days of week histogram count')\n",
    "    plt.ylabel('Count for day')\n",
    "    plt.xlabel('Day')\n",
    "    plt.xticks([i for i in range(len(days_week_dict.keys()))],list(days_week_dict.keys()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a6ae8",
   "metadata": {},
   "source": [
    "**BERT MLM answers tokenized twice regarless**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06979968",
   "metadata": {},
   "source": [
    "First lets import the tokenizer and the basic model that we will fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dd76b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d5c7ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_MLM_masked_twice_2015 = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee28449",
   "metadata": {},
   "source": [
    "**Now we tokenize the training df , not the eval , because the eval is feed to the accuracy function as a df not a tokenized list**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae920a70",
   "metadata": {},
   "source": [
    "The function create_riddles_and_answers_mask_position_corrected_masked_twice() return three lists as follows:\n",
    "1. list of the hint and answer masked twice as we said , for the input_ids for the training loop\n",
    "2. list of the hist with the answer for the labels in the next function\n",
    "3. list of answers alone , this is wont be used now but it is used by the accuarcy evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4a418d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_twice_masked , train_twice_answered , train_answers = create_riddles_and_answers_mask_position_corrected_masked_twice(nyt_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f891a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of masked 117537  len of answered 117537  len answers alone 117537\n",
      "Do all have the same length?  True\n"
     ]
    }
   ],
   "source": [
    "print('len of masked',len(train_twice_masked),' len of answered' , len(train_twice_answered),' len answers alone', len(train_answers))\n",
    "print('Do all have the same length? ', len(train_twice_masked) == len(train_twice_answered) == len(train_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0df31f5",
   "metadata": {},
   "source": [
    "Now we will tokenize the training input with the function tokenized_list_for_model() , which will recieve the tokenizer two lists\n",
    "1. one is the list 'train_twice_masked' which will be used for input_ids\n",
    "2. seconed is 'train_twice_answered' which will be used as the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cd5462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_twice_tokenized = tokenized_list_for_model(tokenizer , [train_twice_masked,train_twice_answered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "681341bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train masked twice size is  torch.Size([117537, 44])\n"
     ]
    }
   ],
   "source": [
    "print('train masked twice size is ',train_twice_tokenized.input_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800e05b1",
   "metadata": {},
   "source": [
    "Now we will create a Dataset and a loader for the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc77a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_masked_twice = CrossWordsDataset(train_twice_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8127a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train_twice = torch.utils.data.DataLoader(dataset_train_masked_twice, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7bf336",
   "metadata": {},
   "source": [
    "**Creating the optimizer and the training loop in next cells**\n",
    "\n",
    "We will train the model for 3 epochs , save model after each epoch , after the third epoch we will evaluate the model based on the training and eval df and continue to fine tune two more epochs if neccesary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eea4878",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLM_masked_twice_2015.to(device)\n",
    "model_MLM_masked_twice_2015.train()\n",
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW(model_MLM_masked_twice_2015.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2af9c88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3674 [00:00<?, ?it/s]<ipython-input-9-9f669dd3d5b9>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 3674/3674 [28:39<00:00,  2.14it/s, loss=0.0301]\n",
      "Epoch 1: 100%|██████████| 3674/3674 [28:44<00:00,  2.13it/s, loss=0.943]\n",
      "Epoch 2: 100%|██████████| 3674/3674 [28:43<00:00,  2.13it/s, loss=0.172] \n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader_train_twice, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model_MLM_masked_twice_2015(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    pickle.dump(model_MLM_masked_twice_2015, open('//home//student//project//BERT_MLM_masked_twice_batch32_pad64_2015_data'+'_epoch'+str(epoch)+'.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b324c",
   "metadata": {},
   "source": [
    "**Evaluate the model based on the the training df and eval df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25e0bc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_MLM_masked_twice_2015.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dea42064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4919/4919 [01:57<00:00, 41.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  473 , Count for masked twice was:  1020  both of them sum up to 1493  out of a total of  4919\n",
      "So this brings us to the accuract for top  10  prediction model made at 30.352 %.\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5187/5187 [01:53<00:00, 45.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  653 , Count for masked twice was:  1935  both of them sum up to 2588  out of a total of  5187\n",
      "So this brings us to the accuract for top  10  prediction model made at 49.894 %.\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5176/5176 [01:56<00:00, 44.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  560 , Count for masked twice was:  1653  both of them sum up to 2213  out of a total of  5176\n",
      "So this brings us to the accuract for top  10  prediction model made at 42.755 %.\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4973/4973 [01:51<00:00, 44.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  541 , Count for masked twice was:  1324  both of them sum up to 1865  out of a total of  4973\n",
      "So this brings us to the accuract for top  10  prediction model made at 37.503 %.\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [02:00<00:00, 41.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  519 , Count for masked twice was:  990  both of them sum up to 1509  out of a total of  4986\n",
      "So this brings us to the accuract for top  10  prediction model made at 30.265 %.\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4407/4407 [01:49<00:00, 40.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  381 , Count for masked twice was:  617  both of them sum up to 998  out of a total of  4407\n",
      "So this brings us to the accuract for top  10  prediction model made at 22.646 %.\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [01:46<00:00, 39.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  316 , Count for masked twice was:  435  both of them sum up to 751  out of a total of  4200\n",
      "So this brings us to the accuract for top  10  prediction model made at 17.881 %.\n",
      "Total score as follows\n",
      "Count for masked once was:  3443 , Count for masked twice was:  7974  both of them sum up to 11417  out of a total of  33848\n",
      "So this brings us to the accuract for top  10  prediction model made at 33.73 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results_masked_twice_2015_3epochs = get_top_k_accuracy_by_days(model_MLM_masked_twice_2015,tokenizer,nyt_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f062314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17163/17163 [05:56<00:00, 48.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  1392 , Count for masked twice was:  8032  both of them sum up to 9424  out of a total of  17163\n",
      "So this brings us to the accuract for top  10  prediction model made at 54.909 %.\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17980/17980 [05:32<00:00, 54.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  1100 , Count for masked twice was:  11279  both of them sum up to 12379  out of a total of  17980\n",
      "So this brings us to the accuract for top  10  prediction model made at 68.849 %.\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17594/17594 [05:40<00:00, 51.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  1314 , Count for masked twice was:  10024  both of them sum up to 11338  out of a total of  17594\n",
      "So this brings us to the accuract for top  10  prediction model made at 64.442 %.\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17358/17358 [05:42<00:00, 50.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  1324 , Count for masked twice was:  9328  both of them sum up to 10652  out of a total of  17358\n",
      "So this brings us to the accuract for top  10  prediction model made at 61.367 %.\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17076/17076 [05:51<00:00, 48.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  1545 , Count for masked twice was:  7891  both of them sum up to 9436  out of a total of  17076\n",
      "So this brings us to the accuract for top  10  prediction model made at 55.259 %.\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15519/15519 [05:34<00:00, 46.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  1380 , Count for masked twice was:  6138  both of them sum up to 7518  out of a total of  15519\n",
      "So this brings us to the accuract for top  10  prediction model made at 48.444 %.\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14847/14847 [05:28<00:00, 45.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  1278 , Count for masked twice was:  5174  both of them sum up to 6452  out of a total of  14847\n",
      "So this brings us to the accuract for top  10  prediction model made at 43.457 %.\n",
      "Total score as follows\n",
      "Count for masked once was:  9333 , Count for masked twice was:  57866  both of them sum up to 67199  out of a total of  117537\n",
      "So this brings us to the accuract for top  10  prediction model made at 57.173 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_results_masked_twice_2015_3epochs = get_top_k_accuracy_by_days(model_MLM_masked_twice_2015,tokenizer,nyt_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cac5b0",
   "metadata": {},
   "source": [
    "**We will fine tune the model for two more epochs and see where that gets us**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fc2b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLM_masked_twice_2015.train()\n",
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW(model_MLM_masked_twice_2015.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d41c2867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3674 [00:00<?, ?it/s]<ipython-input-9-9f669dd3d5b9>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 3674/3674 [28:41<00:00,  2.13it/s, loss=0.225] \n",
      "Epoch 1: 100%|██████████| 3674/3674 [28:42<00:00,  2.13it/s, loss=0.0312]\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader_train_twice, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model_MLM_masked_twice_2015(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    pickle.dump(model_MLM_masked_twice_2015, open('//home//student//project//BERT_MLM_masked_twice_batch32_pad64_2015_data'+'_epoch'+str(epoch+3)+'.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e27b0b",
   "metadata": {},
   "source": [
    "**Back to eval after 5 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b750f5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_MLM_masked_twice_2015.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66d0eb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4919/4919 [01:55<00:00, 42.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  375 , Count for masked twice was:  1259  both of them sum up to 1634  out of a total of  4919\n",
      "So this brings us to the accuract for top  10  prediction model made at 33.218 %.\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5187/5187 [01:50<00:00, 46.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  527 , Count for masked twice was:  2160  both of them sum up to 2687  out of a total of  5187\n",
      "So this brings us to the accuract for top  10  prediction model made at 51.803 %.\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5176/5176 [01:53<00:00, 45.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  484 , Count for masked twice was:  1861  both of them sum up to 2345  out of a total of  5176\n",
      "So this brings us to the accuract for top  10  prediction model made at 45.305 %.\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4973/4973 [01:50<00:00, 45.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  476 , Count for masked twice was:  1477  both of them sum up to 1953  out of a total of  4973\n",
      "So this brings us to the accuract for top  10  prediction model made at 39.272 %.\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [01:56<00:00, 42.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  426 , Count for masked twice was:  1210  both of them sum up to 1636  out of a total of  4986\n",
      "So this brings us to the accuract for top  10  prediction model made at 32.812 %.\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4407/4407 [01:46<00:00, 41.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  273 , Count for masked twice was:  805  both of them sum up to 1078  out of a total of  4407\n",
      "So this brings us to the accuract for top  10  prediction model made at 24.461 %.\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [01:44<00:00, 40.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  265 , Count for masked twice was:  593  both of them sum up to 858  out of a total of  4200\n",
      "So this brings us to the accuract for top  10  prediction model made at 20.429 %.\n",
      "Total score as follows\n",
      "Count for masked once was:  2826 , Count for masked twice was:  9365  both of them sum up to 12191  out of a total of  33848\n",
      "So this brings us to the accuract for top  10  prediction model made at 36.017 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results_masked_twice_2015_5epochs = get_top_k_accuracy_by_days(model_MLM_masked_twice_2015,tokenizer,nyt_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2514a102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17163/17163 [05:04<00:00, 56.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  654 , Count for masked twice was:  11841  both of them sum up to 12495  out of a total of  17163\n",
      "So this brings us to the accuract for top  10  prediction model made at 72.802 %.\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17980/17980 [04:53<00:00, 61.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  532 , Count for masked twice was:  14097  both of them sum up to 14629  out of a total of  17980\n",
      "So this brings us to the accuract for top  10  prediction model made at 81.363 %.\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17594/17594 [04:55<00:00, 59.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  546 , Count for masked twice was:  13391  both of them sum up to 13937  out of a total of  17594\n",
      "So this brings us to the accuract for top  10  prediction model made at 79.215 %.\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17358/17358 [04:54<00:00, 58.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  616 , Count for masked twice was:  12754  both of them sum up to 13370  out of a total of  17358\n",
      "So this brings us to the accuract for top  10  prediction model made at 77.025 %.\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17076/17076 [05:01<00:00, 56.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  753 , Count for masked twice was:  11773  both of them sum up to 12526  out of a total of  17076\n",
      "So this brings us to the accuract for top  10  prediction model made at 73.354 %.\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15519/15519 [04:45<00:00, 54.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  638 , Count for masked twice was:  9792  both of them sum up to 10430  out of a total of  15519\n",
      "So this brings us to the accuract for top  10  prediction model made at 67.208 %.\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14847/14847 [04:44<00:00, 52.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  639 , Count for masked twice was:  8649  both of them sum up to 9288  out of a total of  14847\n",
      "So this brings us to the accuract for top  10  prediction model made at 62.558 %.\n",
      "Total score as follows\n",
      "Count for masked once was:  4378 , Count for masked twice was:  82297  both of them sum up to 86675  out of a total of  117537\n",
      "So this brings us to the accuract for top  10  prediction model made at 73.743 %.\n"
     ]
    }
   ],
   "source": [
    "train_results_masked_twice_2015_5epochs = get_top_k_accuracy_by_days(model_MLM_masked_twice_2015,tokenizer,nyt_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c80417a",
   "metadata": {},
   "source": [
    "# BERT MLM smart masking\n",
    "\n",
    "Basicaly, smart masking means that we cheat the system abit to see if we get better result and on the basis that we dont know how much tokens doest the tokenizer will split the answer to. \n",
    "\n",
    "So what is a smart masking , so when we create the lists of the masked hints while creating the masked hint , contrary to what we have done on the double masking which is just add '[MASK] [MASK]' to where the answer should be in the hint .\n",
    "In the smart masking we first tokenize the answer , and wee how much tokens doest the okenizer return .\n",
    "\n",
    "i.e. the answer is 'asis' and the tokenizer tokinezed it to '[101] [546] [897] [102]' ,(two middle token are made up) we know now that the correct masking for the word containes two tokens  , and we will add '[MASK] [MASK]' to the hint.\n",
    "\n",
    "this goes for both ways , if a word is masked in three token or in one , this will be reflect in the trainig ds masking and thus we hope and predict that the learning will be more effective and we hope in geting better result with smart masking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282ac87",
   "metadata": {},
   "source": [
    "**Start with importing the new clean model , tokenizer is not imported again because its the same one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4884569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "BERT_MLM_smart_masking = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c752b72",
   "metadata": {},
   "source": [
    "**Auxiliary functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f14e37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks_for_hist(answer_len):\n",
    "    tmp = ''\n",
    "    for i in range(answer_len):\n",
    "        tmp += '[MASK]'\n",
    "        if i < answer_len-1:\n",
    "              tmp += ' '\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fde443aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_answered_lists_for_tokenization(df,tokenizer,padding_len = 64):\n",
    "    # we are using smart masking which means first we check how many tokens is the answer in the model and than we add this\n",
    "    # number of masks to the hint in the correct position , say the answer is tokenized into 3 token than the hint answered will\n",
    "    # be hist : [MASK] [MASK] [MASK]\n",
    "    hint_masked_list = []\n",
    "    hint_answered_list = []\n",
    "    answers_list = []\n",
    "    for row in df.itertuples():\n",
    "        tokenized_answer_len = 0\n",
    "        tokenized_answer = tokenizer(str(row.Word),padding = True,truncation = True,max_length = padding_len , return_tensors = 'pt')\n",
    "        tokenized_answer_len = len(tokenized_answer.input_ids[0].tolist())-2 # because it will always be [101 , x , x , 102]\n",
    "        masks_for_hist = create_masks_for_hist(tokenized_answer_len)\n",
    "        if '_' in str(row.Clue):\n",
    "            masked = re.sub('___',masks_for_hist,row.Clue)\n",
    "            answered = re.sub('___',str(row.Word),row.Clue)\n",
    "        else:\n",
    "            masked = str(row.Clue) + ' : ' + masks_for_hist\n",
    "            answered = str(row.Clue) + ' : ' + str(row.Word)\n",
    "        hint_masked_list.append(masked)\n",
    "        hint_answered_list.append(answered)\n",
    "        answers_list.append(str(row.Word))\n",
    "    return hint_masked_list,hint_answered_list,answers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5047d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_accuracy_by_days_smart_masking(model,tokenizer,df_to_check,top_ks = 10):\n",
    "    days = ['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday']\n",
    "    results_dict = {}\n",
    "    overall_cnt = 0\n",
    "    overall_total = 0\n",
    "    for day in days:\n",
    "        cnt = 0\n",
    "        total = 0\n",
    "        masked_by_day , answered_by_day, asnwers_day = create_masked_answered_lists_for_tokenization(df_to_check[df_to_check['day_of_week'] == day],tokenizer)\n",
    "        day_inputs = tokenized_list_for_model(tokenizer , [masked_by_day,answered_by_day])\n",
    "        print(day,' The results are as follows: ')\n",
    "        cnt , total = get_topk_accuracy_smart_masking(model , tokenizer , day_inputs,asnwers_day,top_ks)\n",
    "        results_dict[day] = {'total_day' : total , 'cnt_day' : cnt }\n",
    "        overall_cnt += cnt\n",
    "        overall_total += total\n",
    "    results_dict['overall'] = {'total_enteries' : overall_total , 'total_cnt' : overall_cnt}\n",
    "    print('Overall results for df as follows\\n')\n",
    "    print('Count for top ',top_ks,'predictions is ',overall_cnt,' out of total of ',overall_total,' enteries.')\n",
    "    print('This gives us an accuracy precentage of',round((overall_cnt/overall_total)*100,3))\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9f0d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_accuracy_smart_masking(model,tokenizer,masked_tokenized , answered_untokenzed_list,top_ks = 10):\n",
    "    cnt = 0\n",
    "    total = len(answered_untokenzed_list)\n",
    "    loop = tqdm(range(total), leave=True)\n",
    "    for i in loop:\n",
    "        answer = answered_untokenzed_list[i]\n",
    "        input_ids = masked_tokenized.input_ids[i].to(model.device).unsqueeze(0)\n",
    "        token_type_ids = masked_tokenized.token_type_ids[i].to(model.device).unsqueeze(0)\n",
    "        attention_masks = masked_tokenized.attention_mask[i].to(model.device).unsqueeze(0)\n",
    "        \n",
    "        output = model(input_ids = input_ids,token_type_ids = token_type_ids ,attention_mask = attention_masks)\n",
    "        \n",
    "        masked_id_list = []\n",
    "        for mask_id in range(masked_tokenized.input_ids[i].shape[0]):\n",
    "            if masked_tokenized.input_ids[i][mask_id] == 103:\n",
    "                  masked_id_list.append(mask_id)\n",
    "        \n",
    "        topk_predictions_indices = output.logits[0][masked_id_list].topk(5).indices\n",
    "        \n",
    "        if ( len(topk_predictions_indices) == 0): # for some reason no masked tokens were found\n",
    "            continue\n",
    "        predictions = []\n",
    "        \n",
    "#         print(len(topk_predictions_indices[0]))\n",
    "        for i in range(len(topk_predictions_indices[0])): # on top k predication\n",
    "            tokens_to_decode = []\n",
    "            for k in range(len(masked_id_list)): # on masked tokens\n",
    "                tokens_to_decode.append(topk_predictions_indices[k][i])\n",
    "            predictions.extend(tokenizer.decode(tokens_to_decode).split())\n",
    "        \n",
    "        if ( answer.lower() in predictions):\n",
    "            cnt += 1\n",
    "    print('Count for top ',top_ks,'predictions is ',cnt,' out of total of ',total,' enteries.')\n",
    "    print('This gives us an accuracy precentage of',round((cnt/total)*100,3))\n",
    "    return cnt,total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7bfd40",
   "metadata": {},
   "source": [
    "**Tokenizing the train df for smart masking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "322d8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_smart_train , answered_smart_train , answers_smart_list_train = create_masked_answered_lists_for_tokenization(nyt_train_df,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d086a1e",
   "metadata": {},
   "source": [
    "As before , the function returns three lists\n",
    "1. first is the hint masked using smart masking - 'masked_smart_train'\n",
    "2. second is hint with the answer in the same position as the mask - 'answered_smart_train'\n",
    "3. is the answers only list , gets a different name for differantiation , not different at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c01fb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets see if the shape of the lists are the same:  True\n"
     ]
    }
   ],
   "source": [
    "print('Lets see if the shape of the lists are the same: ',len(masked_smart_train) == len(answered_smart_train) == len(answers_smart_list_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f70e2",
   "metadata": {},
   "source": [
    "**Tokenize the lists as before , nothing different in thetokenization procedure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c6bd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_smart_masked = tokenized_list_for_model(tokenizer ,[masked_smart_train,answered_smart_train] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfb4b5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the tokenized train ds just to be suure torch.Size([117537, 44])\n"
     ]
    }
   ],
   "source": [
    "print('shape of the tokenized train ds just to be suure',tokenized_train_smart_masked.input_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e432480",
   "metadata": {},
   "source": [
    "**Creating the trtaining loop and its dataloaders and optimizers**\n",
    "\n",
    "As before , we train for 3 epochs , get accuracy on eval and train and train some more if needed , this time because the masking is done smartly we do not predict the traning proccess will requre more than 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "933e41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_smart = CrossWordsDataset(tokenized_train_smart_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55ed2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train_smart = torch.utils.data.DataLoader(dataset_train_smart, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48cd9aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MLM_smart_masking.to(device)\n",
    "BERT_MLM_smart_masking.train()\n",
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW(BERT_MLM_smart_masking.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edac32b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3674 [00:00<?, ?it/s]<ipython-input-9-9f669dd3d5b9>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 3674/3674 [28:42<00:00,  2.13it/s, loss=0.336]\n",
      "Epoch 1: 100%|██████████| 3674/3674 [28:45<00:00,  2.13it/s, loss=0.0415]\n",
      "Epoch 2: 100%|██████████| 3674/3674 [28:44<00:00,  2.13it/s, loss=0.15]  \n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader_train_smart, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = BERT_MLM_smart_masking(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    pickle.dump(BERT_MLM_smart_masking, open('//home//student//project//BERT_MLM_smart_masking_batch32_pad64_2015'+'_epoch'+str(epoch)+'.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef564f44",
   "metadata": {},
   "source": [
    "**lets evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "417dac1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERT_MLM_smart_masking.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebae9d59",
   "metadata": {},
   "source": [
    "First we will evaluate the eval and train df with the accuracy function for the previous model and than with the new function for the smart masking\n",
    "Generallly speaking they are not different just the second was created for smart masking and is abit faster and prints abit differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b0284fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4919/4919 [02:00<00:00, 40.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  965 , Count for masked twice was:  871  both of them sum up to 1836  out of a total of  4919\n",
      "So this brings us to the accuract for top  10  prediction model made at 37.325 %.\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5187/5187 [01:58<00:00, 43.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  1349 , Count for masked twice was:  1585  both of them sum up to 2934  out of a total of  5187\n",
      "So this brings us to the accuract for top  10  prediction model made at 56.564 %.\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5176/5176 [02:00<00:00, 42.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  1231 , Count for masked twice was:  1396  both of them sum up to 2627  out of a total of  5176\n",
      "So this brings us to the accuract for top  10  prediction model made at 50.753 %.\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4973/4973 [01:56<00:00, 42.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  1163 , Count for masked twice was:  1042  both of them sum up to 2205  out of a total of  4973\n",
      "So this brings us to the accuract for top  10  prediction model made at 44.339 %.\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [02:02<00:00, 40.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  1016 , Count for masked twice was:  826  both of them sum up to 1842  out of a total of  4986\n",
      "So this brings us to the accuract for top  10  prediction model made at 36.943 %.\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4407/4407 [01:51<00:00, 39.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  741 , Count for masked twice was:  537  both of them sum up to 1278  out of a total of  4407\n",
      "So this brings us to the accuract for top  10  prediction model made at 28.999 %.\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [01:48<00:00, 38.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  628 , Count for masked twice was:  387  both of them sum up to 1015  out of a total of  4200\n",
      "So this brings us to the accuract for top  10  prediction model made at 24.167 %.\n",
      "Total score as follows\n",
      "Count for masked once was:  7093 , Count for masked twice was:  6644  both of them sum up to 13737  out of a total of  33848\n",
      "So this brings us to the accuract for top  10  prediction model made at 40.584 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results_smart_masked_2015_3epochs_old_acc = get_top_k_accuracy_by_days(BERT_MLM_smart_masking,tokenizer,nyt_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fdb555a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17163/17163 [06:09<00:00, 46.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  4574 , Count for masked twice was:  6876  both of them sum up to 11450  out of a total of  17163\n",
      "So this brings us to the accuract for top  10  prediction model made at 66.713 %.\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17980/17980 [05:55<00:00, 50.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  4633 , Count for masked twice was:  9356  both of them sum up to 13989  out of a total of  17980\n",
      "So this brings us to the accuract for top  10  prediction model made at 77.803 %.\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17594/17594 [06:01<00:00, 48.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  4722 , Count for masked twice was:  8491  both of them sum up to 13213  out of a total of  17594\n",
      "So this brings us to the accuract for top  10  prediction model made at 75.099 %.\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17358/17358 [06:05<00:00, 47.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  4948 , Count for masked twice was:  7560  both of them sum up to 12508  out of a total of  17358\n",
      "So this brings us to the accuract for top  10  prediction model made at 72.059 %.\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17076/17076 [06:13<00:00, 45.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  5004 , Count for masked twice was:  6444  both of them sum up to 11448  out of a total of  17076\n",
      "So this brings us to the accuract for top  10  prediction model made at 67.041 %.\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15519/15519 [05:50<00:00, 44.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  4369 , Count for masked twice was:  5024  both of them sum up to 9393  out of a total of  15519\n",
      "So this brings us to the accuract for top  10  prediction model made at 60.526 %.\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14847/14847 [05:39<00:00, 43.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  3840 , Count for masked twice was:  4429  both of them sum up to 8269  out of a total of  14847\n",
      "So this brings us to the accuract for top  10  prediction model made at 55.695 %.\n",
      "Total score as follows\n",
      "Count for masked once was:  32090 , Count for masked twice was:  48180  both of them sum up to 80270  out of a total of  117537\n",
      "So this brings us to the accuract for top  10  prediction model made at 68.293 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_results_smart_masked_2015_3epochs_old_acc = get_top_k_accuracy_by_days(BERT_MLM_smart_masking,tokenizer,nyt_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446c00f",
   "metadata": {},
   "source": [
    "**Now lets use the new accuracy function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc30a864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4919/4919 [01:06<00:00, 74.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1710  out of total of  4919  enteries.\n",
      "This gives us an accuracy precentage of 34.763\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5187/5187 [01:09<00:00, 74.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  2867  out of total of  5187  enteries.\n",
      "This gives us an accuracy precentage of 55.273\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5176/5176 [01:08<00:00, 75.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  2541  out of total of  5176  enteries.\n",
      "This gives us an accuracy precentage of 49.092\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4973/4973 [01:04<00:00, 77.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  2098  out of total of  4973  enteries.\n",
      "This gives us an accuracy precentage of 42.188\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [01:06<00:00, 74.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1733  out of total of  4986  enteries.\n",
      "This gives us an accuracy precentage of 34.757\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4407/4407 [00:59<00:00, 74.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1185  out of total of  4407  enteries.\n",
      "This gives us an accuracy precentage of 26.889\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [00:56<00:00, 74.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  910  out of total of  4200  enteries.\n",
      "This gives us an accuracy precentage of 21.667\n",
      "Overall results for df as follows\n",
      "\n",
      "Count for top  10 predictions is  13044  out of total of  33848  enteries.\n",
      "This gives us an accuracy precentage of 38.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_df_results_3epochs_smart_masking_acc_2015 = get_top_k_accuracy_by_days_smart_masking(BERT_MLM_smart_masking,tokenizer , nyt_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24820b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17163/17163 [03:50<00:00, 74.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  11669  out of total of  17163  enteries.\n",
      "This gives us an accuracy precentage of 67.989\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17980/17980 [03:59<00:00, 74.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  14318  out of total of  17980  enteries.\n",
      "This gives us an accuracy precentage of 79.633\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17594/17594 [03:56<00:00, 74.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  13470  out of total of  17594  enteries.\n",
      "This gives us an accuracy precentage of 76.56\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17358/17358 [03:51<00:00, 74.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  12723  out of total of  17358  enteries.\n",
      "This gives us an accuracy precentage of 73.298\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17076/17076 [03:49<00:00, 74.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  11620  out of total of  17076  enteries.\n",
      "This gives us an accuracy precentage of 68.049\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15519/15519 [03:27<00:00, 74.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  9505  out of total of  15519  enteries.\n",
      "This gives us an accuracy precentage of 61.248\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14847/14847 [03:19<00:00, 74.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  8383  out of total of  14847  enteries.\n",
      "This gives us an accuracy precentage of 56.463\n",
      "Overall results for df as follows\n",
      "\n",
      "Count for top  10 predictions is  81688  out of total of  117537  enteries.\n",
      "This gives us an accuracy precentage of 69.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_results_3epochs_smart_masking_acc_2015 = get_top_k_accuracy_by_days_smart_masking(BERT_MLM_smart_masking,tokenizer , nyt_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a5094",
   "metadata": {},
   "source": [
    "**Well, the result are not that bad on the train , but the eval results arenot as good , only on monday we can see a adequate results.\n",
    "Monday is the easiest crossword of the week**\n",
    "\n",
    "**Lets train the smart masking model for two more epochs , after we will try the smart masking model after 2 epochs because it has alow loss score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e97ab974",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MLM_smart_masking.to(device)\n",
    "BERT_MLM_smart_masking.train()\n",
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW(BERT_MLM_smart_masking.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1975348f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3674 [00:00<?, ?it/s]<ipython-input-9-9f669dd3d5b9>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 3674/3674 [28:41<00:00,  2.13it/s, loss=0.00371]\n",
      "Epoch 1: 100%|██████████| 3674/3674 [28:42<00:00,  2.13it/s, loss=0.0897]\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader_train_smart, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = BERT_MLM_smart_masking(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    pickle.dump(BERT_MLM_smart_masking, open('//home//student//project//BERT_MLM_smart_masking_batch32_pad64_2015'+'_epoch'+str(epoch+3)+'.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae78da7",
   "metadata": {},
   "source": [
    "Now back to evaluating the eval and train df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75f6c88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERT_MLM_smart_masking.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9336316",
   "metadata": {},
   "source": [
    "Firstly with old twice and single masking technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eae8bd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4919/4919 [01:56<00:00, 42.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  789 , Count for masked twice was:  1198  both of them sum up to 1987  out of a total of  4919\n",
      "So this brings us to the accuract for top  10  prediction model made at 40.394 %.\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5187/5187 [01:52<00:00, 46.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  1081 , Count for masked twice was:  2014  both of them sum up to 3095  out of a total of  5187\n",
      "So this brings us to the accuract for top  10  prediction model made at 59.668 %.\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5176/5176 [01:55<00:00, 44.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  977 , Count for masked twice was:  1765  both of them sum up to 2742  out of a total of  5176\n",
      "So this brings us to the accuract for top  10  prediction model made at 52.975 %.\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4973/4973 [01:52<00:00, 44.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  956 , Count for masked twice was:  1363  both of them sum up to 2319  out of a total of  4973\n",
      "So this brings us to the accuract for top  10  prediction model made at 46.632 %.\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [01:58<00:00, 42.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  866 , Count for masked twice was:  1129  both of them sum up to 1995  out of a total of  4986\n",
      "So this brings us to the accuract for top  10  prediction model made at 40.012 %.\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4407/4407 [01:49<00:00, 40.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  637 , Count for masked twice was:  712  both of them sum up to 1349  out of a total of  4407\n",
      "So this brings us to the accuract for top  10  prediction model made at 30.61 %.\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [01:45<00:00, 39.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  513 , Count for masked twice was:  551  both of them sum up to 1064  out of a total of  4200\n",
      "So this brings us to the accuract for top  10  prediction model made at 25.333 %.\n",
      "Total score as follows\n",
      "Count for masked once was:  5819 , Count for masked twice was:  8732  both of them sum up to 14551  out of a total of  33848\n",
      "So this brings us to the accuract for top  10  prediction model made at 42.989 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results_smart_masked_2015_5epochs_old_acc = get_top_k_accuracy_by_days(BERT_MLM_smart_masking,tokenizer,nyt_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d400df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17163/17163 [05:14<00:00, 54.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  2838 , Count for masked twice was:  11228  both of them sum up to 14066  out of a total of  17163\n",
      "So this brings us to the accuract for top  10  prediction model made at 81.955 %.\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17980/17980 [05:10<00:00, 57.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  2562 , Count for masked twice was:  12992  both of them sum up to 15554  out of a total of  17980\n",
      "So this brings us to the accuract for top  10  prediction model made at 86.507 %.\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17594/17594 [05:10<00:00, 56.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  2702 , Count for masked twice was:  12432  both of them sum up to 15134  out of a total of  17594\n",
      "So this brings us to the accuract for top  10  prediction model made at 86.018 %.\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17358/17358 [05:11<00:00, 55.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  2954 , Count for masked twice was:  11809  both of them sum up to 14763  out of a total of  17358\n",
      "So this brings us to the accuract for top  10  prediction model made at 85.05 %.\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17076/17076 [05:16<00:00, 53.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  3189 , Count for masked twice was:  10793  both of them sum up to 13982  out of a total of  17076\n",
      "So this brings us to the accuract for top  10  prediction model made at 81.881 %.\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15519/15519 [04:57<00:00, 52.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  2784 , Count for masked twice was:  9174  both of them sum up to 11958  out of a total of  15519\n",
      "So this brings us to the accuract for top  10  prediction model made at 77.054 %.\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14847/14847 [04:46<00:00, 51.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  2569 , Count for masked twice was:  8479  both of them sum up to 11048  out of a total of  14847\n",
      "So this brings us to the accuract for top  10  prediction model made at 74.412 %.\n",
      "Total score as follows\n",
      "Count for masked once was:  19598 , Count for masked twice was:  76907  both of them sum up to 96505  out of a total of  117537\n",
      "So this brings us to the accuract for top  10  prediction model made at 82.106 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_results_smart_masked_2015_5epochs_old_acc = get_top_k_accuracy_by_days(BERT_MLM_smart_masking,tokenizer,nyt_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e89ea0",
   "metadata": {},
   "source": [
    "Now lets evaluate using accuracy function made for smart msking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25f704a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4919/4919 [01:06<00:00, 74.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1896  out of total of  4919  enteries.\n",
      "This gives us an accuracy precentage of 38.544\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5187/5187 [01:09<00:00, 74.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  3025  out of total of  5187  enteries.\n",
      "This gives us an accuracy precentage of 58.319\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5176/5176 [01:08<00:00, 75.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  2680  out of total of  5176  enteries.\n",
      "This gives us an accuracy precentage of 51.777\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4973/4973 [01:04<00:00, 76.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  2268  out of total of  4973  enteries.\n",
      "This gives us an accuracy precentage of 45.606\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [01:07<00:00, 74.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1890  out of total of  4986  enteries.\n",
      "This gives us an accuracy precentage of 37.906\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4407/4407 [00:59<00:00, 74.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1278  out of total of  4407  enteries.\n",
      "This gives us an accuracy precentage of 28.999\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [00:55<00:00, 75.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1011  out of total of  4200  enteries.\n",
      "This gives us an accuracy precentage of 24.071\n",
      "Overall results for df as follows\n",
      "\n",
      "Count for top  10 predictions is  14048  out of total of  33848  enteries.\n",
      "This gives us an accuracy precentage of 41.503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_df_results_5epochs_smart_masking_acc_2015 = get_top_k_accuracy_by_days_smart_masking(BERT_MLM_smart_masking,tokenizer , nyt_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f5e3326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17163/17163 [03:49<00:00, 74.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  15006  out of total of  17163  enteries.\n",
      "This gives us an accuracy precentage of 87.432\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17980/17980 [04:01<00:00, 74.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  16448  out of total of  17980  enteries.\n",
      "This gives us an accuracy precentage of 91.479\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17594/17594 [03:55<00:00, 74.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  15969  out of total of  17594  enteries.\n",
      "This gives us an accuracy precentage of 90.764\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17358/17358 [03:53<00:00, 74.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  15617  out of total of  17358  enteries.\n",
      "This gives us an accuracy precentage of 89.97\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17076/17076 [03:48<00:00, 74.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  14865  out of total of  17076  enteries.\n",
      "This gives us an accuracy precentage of 87.052\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15519/15519 [03:29<00:00, 74.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  12880  out of total of  15519  enteries.\n",
      "This gives us an accuracy precentage of 82.995\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14847/14847 [03:19<00:00, 74.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  11996  out of total of  14847  enteries.\n",
      "This gives us an accuracy precentage of 80.797\n",
      "Overall results for df as follows\n",
      "\n",
      "Count for top  10 predictions is  102781  out of total of  117537  enteries.\n",
      "This gives us an accuracy precentage of 87.446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_results_5epochs_smart_masking_acc_2015 = get_top_k_accuracy_by_days_smart_masking(BERT_MLM_smart_masking,tokenizer , nyt_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf320f85",
   "metadata": {},
   "source": [
    "**Lets check if the accuracy precentage is different with the smart masking function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2741908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4919/4919 [01:05<00:00, 75.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1505  out of total of  4919  enteries.\n",
      "This gives us an accuracy precentage of 30.596\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5187/5187 [01:09<00:00, 75.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  2547  out of total of  5187  enteries.\n",
      "This gives us an accuracy precentage of 49.104\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5176/5176 [01:08<00:00, 75.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  2174  out of total of  5176  enteries.\n",
      "This gives us an accuracy precentage of 42.002\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4973/4973 [01:03<00:00, 77.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1802  out of total of  4973  enteries.\n",
      "This gives us an accuracy precentage of 36.236\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [01:07<00:00, 74.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1502  out of total of  4986  enteries.\n",
      "This gives us an accuracy precentage of 30.124\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4407/4407 [00:59<00:00, 74.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  969  out of total of  4407  enteries.\n",
      "This gives us an accuracy precentage of 21.988\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [00:56<00:00, 74.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  751  out of total of  4200  enteries.\n",
      "This gives us an accuracy precentage of 17.881\n",
      "Overall results for df as follows\n",
      "\n",
      "Count for top  10 predictions is  11250  out of total of  33848  enteries.\n",
      "This gives us an accuracy precentage of 33.237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results_masked_twice_2015_5epochs_smart_func_acc = get_top_k_accuracy_by_days_smart_masking(model_MLM_masked_twice_2015,tokenizer,nyt_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3607fdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17163/17163 [03:51<00:00, 74.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  12290  out of total of  17163  enteries.\n",
      "This gives us an accuracy precentage of 71.608\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17980/17980 [04:02<00:00, 74.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  14532  out of total of  17980  enteries.\n",
      "This gives us an accuracy precentage of 80.823\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17594/17594 [03:54<00:00, 74.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  13810  out of total of  17594  enteries.\n",
      "This gives us an accuracy precentage of 78.493\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17358/17358 [03:53<00:00, 74.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  13182  out of total of  17358  enteries.\n",
      "This gives us an accuracy precentage of 75.942\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17076/17076 [03:49<00:00, 74.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  12298  out of total of  17076  enteries.\n",
      "This gives us an accuracy precentage of 72.019\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15519/15519 [03:28<00:00, 74.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  10221  out of total of  15519  enteries.\n",
      "This gives us an accuracy precentage of 65.861\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14847/14847 [03:17<00:00, 75.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  9062  out of total of  14847  enteries.\n",
      "This gives us an accuracy precentage of 61.036\n",
      "Overall results for df as follows\n",
      "\n",
      "Count for top  10 predictions is  85395  out of total of  117537  enteries.\n",
      "This gives us an accuracy precentage of 72.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_results_masked_twice_2015_5epochs_smart_func_acc = get_top_k_accuracy_by_days_smart_masking(model_MLM_masked_twice_2015,tokenizer,nyt_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b0ff5",
   "metadata": {},
   "source": [
    "# Evaluating not fune-tuned BERT MLM \n",
    "\n",
    "Now we will see the top 10 accuracy precentage a non fine-tuned BERT MLM model can reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "00e4ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "BERT_clean = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6021d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERT_clean.to(device)\n",
    "BERT_clean.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e101e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4919/4919 [02:10<00:00, 37.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  9 , Count for masked twice was:  69  both of them sum up to 78  out of a total of  4919\n",
      "So this brings us to the accuract for top  10  prediction model made at 1.586 %.\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5187/5187 [02:15<00:00, 38.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  31 , Count for masked twice was:  260  both of them sum up to 291  out of a total of  5187\n",
      "So this brings us to the accuract for top  10  prediction model made at 5.61 %.\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5176/5176 [02:16<00:00, 37.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  16 , Count for masked twice was:  151  both of them sum up to 167  out of a total of  5176\n",
      "So this brings us to the accuract for top  10  prediction model made at 3.226 %.\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4973/4973 [02:07<00:00, 39.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  12 , Count for masked twice was:  115  both of them sum up to 127  out of a total of  4973\n",
      "So this brings us to the accuract for top  10  prediction model made at 2.554 %.\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [02:12<00:00, 37.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  11 , Count for masked twice was:  95  both of them sum up to 106  out of a total of  4986\n",
      "So this brings us to the accuract for top  10  prediction model made at 2.126 %.\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4407/4407 [01:58<00:00, 37.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  10 , Count for masked twice was:  53  both of them sum up to 63  out of a total of  4407\n",
      "So this brings us to the accuract for top  10  prediction model made at 1.43 %.\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [01:51<00:00, 37.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  7 , Count for masked twice was:  51  both of them sum up to 58  out of a total of  4200\n",
      "So this brings us to the accuract for top  10  prediction model made at 1.381 %.\n",
      "Total score as follows\n",
      "Count for masked once was:  96 , Count for masked twice was:  794  both of them sum up to 890  out of a total of  33848\n",
      "So this brings us to the accuract for top  10  prediction model made at 2.629 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_df_results_bert_basic_2015_data_old_acc = get_top_k_accuracy_by_days(BERT_clean,tokenizer,nyt_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4e50e1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17163/17163 [07:36<00:00, 37.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  34 , Count for masked twice was:  294  both of them sum up to 328  out of a total of  17163\n",
      "So this brings us to the accuract for top  10  prediction model made at 1.911 %.\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17980/17980 [07:43<00:00, 38.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  76 , Count for masked twice was:  803  both of them sum up to 879  out of a total of  17980\n",
      "So this brings us to the accuract for top  10  prediction model made at 4.889 %.\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17594/17594 [07:45<00:00, 37.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  63 , Count for masked twice was:  484  both of them sum up to 547  out of a total of  17594\n",
      "So this brings us to the accuract for top  10  prediction model made at 3.109 %.\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17358/17358 [07:39<00:00, 37.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  41 , Count for masked twice was:  409  both of them sum up to 450  out of a total of  17358\n",
      "So this brings us to the accuract for top  10  prediction model made at 2.592 %.\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17076/17076 [07:33<00:00, 37.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  31 , Count for masked twice was:  280  both of them sum up to 311  out of a total of  17076\n",
      "So this brings us to the accuract for top  10  prediction model made at 1.821 %.\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15519/15519 [06:52<00:00, 37.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  29 , Count for masked twice was:  203  both of them sum up to 232  out of a total of  15519\n",
      "So this brings us to the accuract for top  10  prediction model made at 1.495 %.\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14847/14847 [06:35<00:00, 37.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  25 , Count for masked twice was:  153  both of them sum up to 178  out of a total of  14847\n",
      "So this brings us to the accuract for top  10  prediction model made at 1.199 %.\n",
      "Total score as follows\n",
      "Count for masked once was:  299 , Count for masked twice was:  2626  both of them sum up to 2925  out of a total of  117537\n",
      "So this brings us to the accuract for top  10  prediction model made at 2.489 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_results_bert_basic_2015_data_old_acc = get_top_k_accuracy_by_days(BERT_clean,tokenizer,nyt_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb697585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4919/4919 [01:05<00:00, 74.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  50  out of total of  4919  enteries.\n",
      "This gives us an accuracy precentage of 1.016\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5187/5187 [01:09<00:00, 74.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  165  out of total of  5187  enteries.\n",
      "This gives us an accuracy precentage of 3.181\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5176/5176 [01:08<00:00, 75.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  89  out of total of  5176  enteries.\n",
      "This gives us an accuracy precentage of 1.719\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4973/4973 [01:04<00:00, 76.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  69  out of total of  4973  enteries.\n",
      "This gives us an accuracy precentage of 1.387\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [01:06<00:00, 74.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  57  out of total of  4986  enteries.\n",
      "This gives us an accuracy precentage of 1.143\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4407/4407 [00:59<00:00, 74.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  29  out of total of  4407  enteries.\n",
      "This gives us an accuracy precentage of 0.658\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [00:56<00:00, 74.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  33  out of total of  4200  enteries.\n",
      "This gives us an accuracy precentage of 0.786\n",
      "Overall results for df as follows\n",
      "\n",
      "Count for top  10 predictions is  492  out of total of  33848  enteries.\n",
      "This gives us an accuracy precentage of 1.454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_df_results_bert_basic_smart_masking_2015_data = get_top_k_accuracy_by_days_smart_masking(BERT_clean,tokenizer , nyt_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21bec689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17163/17163 [03:50<00:00, 74.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  185  out of total of  17163  enteries.\n",
      "This gives us an accuracy precentage of 1.078\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17980/17980 [04:00<00:00, 74.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  481  out of total of  17980  enteries.\n",
      "This gives us an accuracy precentage of 2.675\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17594/17594 [03:54<00:00, 74.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  300  out of total of  17594  enteries.\n",
      "This gives us an accuracy precentage of 1.705\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17358/17358 [03:51<00:00, 74.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  235  out of total of  17358  enteries.\n",
      "This gives us an accuracy precentage of 1.354\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17076/17076 [03:48<00:00, 74.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  166  out of total of  17076  enteries.\n",
      "This gives us an accuracy precentage of 0.972\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15519/15519 [03:28<00:00, 74.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  115  out of total of  15519  enteries.\n",
      "This gives us an accuracy precentage of 0.741\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14847/14847 [03:18<00:00, 74.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  90  out of total of  14847  enteries.\n",
      "This gives us an accuracy precentage of 0.606\n",
      "Overall results for df as follows\n",
      "\n",
      "Count for top  10 predictions is  1572  out of total of  117537  enteries.\n",
      "This gives us an accuracy precentage of 1.337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_results_bert_basic_smart_masking_2015_data = get_top_k_accuracy_by_days_smart_masking(BERT_clean,tokenizer , nyt_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c2f264",
   "metadata": {},
   "source": [
    "# Lets run the models on the test set which they never saw before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d06fe",
   "metadata": {},
   "source": [
    "**We will use the 5 epochs models to simulate the world distribution in the strongest model we trained**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c514422e",
   "metadata": {},
   "source": [
    "First load test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "280520af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_test_df = pd.read_csv('//home//student//project//project_final_files//nytcrosswords_balanced_days_test_2015.csv',parse_dates = ['Date'], encoding =\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8f9bc86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Word</th>\n",
       "      <th>Clue</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>SRO</td>\n",
       "      <td>Inits. for a theatrical hit</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>STASIS</td>\n",
       "      <td>State of equilibrium</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>SACHS</td>\n",
       "      <td>Samuel ___, business partner of Marcus Goldman</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>YES</td>\n",
       "      <td>\"You betcha!\"</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>TOETOTOE</td>\n",
       "      <td>Mano a mano</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16679</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>PED</td>\n",
       "      <td>Taxi eschewer, for short</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16680</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>SWANK</td>\n",
       "      <td>Ritzy</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16681</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>PASTE</td>\n",
       "      <td>Wallop</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16682</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>NAFTA</td>\n",
       "      <td>Clinton-backed pact</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16683</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>ICEE</td>\n",
       "      <td>Brain-freezing treat</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16684 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date      Word                                            Clue  \\\n",
       "0     2021-10-31       SRO                     Inits. for a theatrical hit   \n",
       "1     2021-10-31    STASIS                            State of equilibrium   \n",
       "2     2021-10-31     SACHS  Samuel ___, business partner of Marcus Goldman   \n",
       "3     2021-10-31       YES                                   \"You betcha!\"   \n",
       "4     2021-10-31  TOETOTOE                                     Mano a mano   \n",
       "...          ...       ...                                             ...   \n",
       "16679 2015-01-01       PED                        Taxi eschewer, for short   \n",
       "16680 2015-01-01     SWANK                                           Ritzy   \n",
       "16681 2015-01-01     PASTE                                          Wallop   \n",
       "16682 2015-01-01     NAFTA                             Clinton-backed pact   \n",
       "16683 2015-01-01      ICEE                            Brain-freezing treat   \n",
       "\n",
       "      day_of_week  year  \n",
       "0          Sunday  2021  \n",
       "1          Sunday  2021  \n",
       "2          Sunday  2021  \n",
       "3          Sunday  2021  \n",
       "4          Sunday  2021  \n",
       "...           ...   ...  \n",
       "16679    Thursday  2015  \n",
       "16680    Thursday  2015  \n",
       "16681    Thursday  2015  \n",
       "16682    Thursday  2015  \n",
       "16683    Thursday  2015  \n",
       "\n",
       "[16684 rows x 5 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt_test_df.drop(columns = ['Unnamed: 0'],inplace = True)\n",
    "nyt_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eebb632",
   "metadata": {},
   "source": [
    "**First evaluate using the naive masking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "100769c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2451/2451 [00:58<00:00, 41.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  193 , Count for masked twice was:  600  both of them sum up to 793  out of a total of  2451\n",
      "So this brings us to the accuract for top  10  prediction model made at 32.354 %.\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2548/2548 [00:53<00:00, 47.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  249 , Count for masked twice was:  1110  both of them sum up to 1359  out of a total of  2548\n",
      "So this brings us to the accuract for top  10  prediction model made at 53.336 %.\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2511/2511 [00:54<00:00, 46.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  240 , Count for masked twice was:  905  both of them sum up to 1145  out of a total of  2511\n",
      "So this brings us to the accuract for top  10  prediction model made at 45.599 %.\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2437/2437 [00:54<00:00, 44.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  222 , Count for masked twice was:  776  both of them sum up to 998  out of a total of  2437\n",
      "So this brings us to the accuract for top  10  prediction model made at 40.952 %.\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2491/2491 [00:58<00:00, 42.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  206 , Count for masked twice was:  570  both of them sum up to 776  out of a total of  2491\n",
      "So this brings us to the accuract for top  10  prediction model made at 31.152 %.\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2136/2136 [00:51<00:00, 41.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  163 , Count for masked twice was:  379  both of them sum up to 542  out of a total of  2136\n",
      "So this brings us to the accuract for top  10  prediction model made at 25.375 %.\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2110/2110 [00:52<00:00, 39.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  112 , Count for masked twice was:  303  both of them sum up to 415  out of a total of  2110\n",
      "So this brings us to the accuract for top  10  prediction model made at 19.668 %.\n",
      "Total score as follows\n",
      "Count for masked once was:  1385 , Count for masked twice was:  4643  both of them sum up to 6028  out of a total of  16684\n",
      "So this brings us to the accuract for top  10  prediction model made at 36.13 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_naive_masking_5epochs_MLM_twice = get_top_k_accuracy_by_days(model_MLM_masked_twice_2015,tokenizer,nyt_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79463aa",
   "metadata": {},
   "source": [
    "**Secondly with the smart masking technique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "02d1067e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2451/2451 [00:33<00:00, 73.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  725  out of total of  2451  enteries.\n",
      "This gives us an accuracy precentage of 29.58\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2548/2548 [00:33<00:00, 77.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1294  out of total of  2548  enteries.\n",
      "This gives us an accuracy precentage of 50.785\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2511/2511 [00:32<00:00, 76.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1057  out of total of  2511  enteries.\n",
      "This gives us an accuracy precentage of 42.095\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2437/2437 [00:31<00:00, 76.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  922  out of total of  2437  enteries.\n",
      "This gives us an accuracy precentage of 37.833\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2491/2491 [00:32<00:00, 76.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  700  out of total of  2491  enteries.\n",
      "This gives us an accuracy precentage of 28.101\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2136/2136 [00:27<00:00, 77.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  489  out of total of  2136  enteries.\n",
      "This gives us an accuracy precentage of 22.893\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2110/2110 [00:28<00:00, 74.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  370  out of total of  2110  enteries.\n",
      "This gives us an accuracy precentage of 17.536\n",
      "Overall results for df as follows\n",
      "\n",
      "Count for top  10 predictions is  5557  out of total of  16684  enteries.\n",
      "This gives us an accuracy precentage of 33.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_smart_masking_5epochs_MLM_smart_masking_BERT = get_top_k_accuracy_by_days_smart_masking(model_MLM_masked_twice_2015,tokenizer,nyt_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "de39d6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2451/2451 [00:58<00:00, 41.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  378 , Count for masked twice was:  566  both of them sum up to 944  out of a total of  2451\n",
      "So this brings us to the accuract for top  10  prediction model made at 38.515 %.\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2548/2548 [00:53<00:00, 47.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  515 , Count for masked twice was:  1011  both of them sum up to 1526  out of a total of  2548\n",
      "So this brings us to the accuract for top  10  prediction model made at 59.89 %.\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2511/2511 [00:55<00:00, 45.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  482 , Count for masked twice was:  822  both of them sum up to 1304  out of a total of  2511\n",
      "So this brings us to the accuract for top  10  prediction model made at 51.932 %.\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2437/2437 [00:54<00:00, 44.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  460 , Count for masked twice was:  687  both of them sum up to 1147  out of a total of  2437\n",
      "So this brings us to the accuract for top  10  prediction model made at 47.066 %.\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2491/2491 [00:58<00:00, 42.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  418 , Count for masked twice was:  536  both of them sum up to 954  out of a total of  2491\n",
      "So this brings us to the accuract for top  10  prediction model made at 38.298 %.\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2136/2136 [00:51<00:00, 41.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  308 , Count for masked twice was:  356  both of them sum up to 664  out of a total of  2136\n",
      "So this brings us to the accuract for top  10  prediction model made at 31.086 %.\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2110/2110 [00:53<00:00, 39.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  268 , Count for masked twice was:  272  both of them sum up to 540  out of a total of  2110\n",
      "So this brings us to the accuract for top  10  prediction model made at 25.592 %.\n",
      "Total score as follows\n",
      "Count for masked once was:  2829 , Count for masked twice was:  4250  both of them sum up to 7079  out of a total of  16684\n",
      "So this brings us to the accuract for top  10  prediction model made at 42.43 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_result_naitve_mask_func_smart_bert_model_5epochs = get_top_k_accuracy_by_days(BERT_MLM_smart_masking,tokenizer,nyt_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "743d2721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2451/2451 [00:32<00:00, 74.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  918  out of total of  2451  enteries.\n",
      "This gives us an accuracy precentage of 37.454\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2548/2548 [00:33<00:00, 76.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1502  out of total of  2548  enteries.\n",
      "This gives us an accuracy precentage of 58.948\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2511/2511 [00:32<00:00, 77.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1280  out of total of  2511  enteries.\n",
      "This gives us an accuracy precentage of 50.976\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2437/2437 [00:31<00:00, 76.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  1102  out of total of  2437  enteries.\n",
      "This gives us an accuracy precentage of 45.22\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2491/2491 [00:32<00:00, 76.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  914  out of total of  2491  enteries.\n",
      "This gives us an accuracy precentage of 36.692\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2136/2136 [00:28<00:00, 75.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  632  out of total of  2136  enteries.\n",
      "This gives us an accuracy precentage of 29.588\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2110/2110 [00:28<00:00, 73.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  504  out of total of  2110  enteries.\n",
      "This gives us an accuracy precentage of 23.886\n",
      "Overall results for df as follows\n",
      "\n",
      "Count for top  10 predictions is  6852  out of total of  16684  enteries.\n",
      "This gives us an accuracy precentage of 41.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_result_smart_mask_func_smart_bert_model_5epochs = get_top_k_accuracy_by_days_smart_masking(BERT_MLM_smart_masking,tokenizer,nyt_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f3e336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2451/2451 [01:05<00:00, 37.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  3 , Count for masked twice was:  51  both of them sum up to 54  out of a total of  2451\n",
      "So this brings us to the accuract for top  10  prediction model made at 2.203 %.\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2548/2548 [01:04<00:00, 39.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  13 , Count for masked twice was:  138  both of them sum up to 151  out of a total of  2548\n",
      "So this brings us to the accuract for top  10  prediction model made at 5.926 %.\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2511/2511 [01:04<00:00, 38.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  7 , Count for masked twice was:  83  both of them sum up to 90  out of a total of  2511\n",
      "So this brings us to the accuract for top  10  prediction model made at 3.584 %.\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2437/2437 [01:03<00:00, 38.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  5 , Count for masked twice was:  60  both of them sum up to 65  out of a total of  2437\n",
      "So this brings us to the accuract for top  10  prediction model made at 2.667 %.\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2491/2491 [01:04<00:00, 38.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  3 , Count for masked twice was:  53  both of them sum up to 56  out of a total of  2491\n",
      "So this brings us to the accuract for top  10  prediction model made at 2.248 %.\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2136/2136 [00:55<00:00, 38.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  6 , Count for masked twice was:  25  both of them sum up to 31  out of a total of  2136\n",
      "So this brings us to the accuract for top  10  prediction model made at 1.451 %.\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2110/2110 [00:56<00:00, 37.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for masked once was:  5 , Count for masked twice was:  22  both of them sum up to 27  out of a total of  2110\n",
      "So this brings us to the accuract for top  10  prediction model made at 1.28 %.\n",
      "Total score as follows\n",
      "Count for masked once was:  42 , Count for masked twice was:  432  both of them sum up to 474  out of a total of  16684\n",
      "So this brings us to the accuract for top  10  prediction model made at 2.841 %.\n"
     ]
    }
   ],
   "source": [
    "test_results_naive_mask_func_MLM_clean_BERT = get_top_k_accuracy_by_days(BERT_clean,tokenizer,nyt_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "96311038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2451/2451 [00:32<00:00, 74.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  29  out of total of  2451  enteries.\n",
      "This gives us an accuracy precentage of 1.183\n",
      "Monday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2548/2548 [00:33<00:00, 77.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  82  out of total of  2548  enteries.\n",
      "This gives us an accuracy precentage of 3.218\n",
      "Tuesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2511/2511 [00:32<00:00, 77.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  55  out of total of  2511  enteries.\n",
      "This gives us an accuracy precentage of 2.19\n",
      "Wednesday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2437/2437 [00:32<00:00, 74.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  32  out of total of  2437  enteries.\n",
      "This gives us an accuracy precentage of 1.313\n",
      "Thursday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2491/2491 [00:32<00:00, 76.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  31  out of total of  2491  enteries.\n",
      "This gives us an accuracy precentage of 1.244\n",
      "Friday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2136/2136 [00:27<00:00, 76.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  17  out of total of  2136  enteries.\n",
      "This gives us an accuracy precentage of 0.796\n",
      "Saturday  The results are as follows: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2110/2110 [00:28<00:00, 74.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for top  10 predictions is  15  out of total of  2110  enteries.\n",
      "This gives us an accuracy precentage of 0.711\n",
      "Overall results for df as follows\n",
      "\n",
      "Count for top  10 predictions is  261  out of total of  16684  enteries.\n",
      "This gives us an accuracy precentage of 1.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_smart_mask_func_MLM_clean_BERT = get_top_k_accuracy_by_days_smart_masking(BERT_clean,tokenizer,nyt_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5ad85",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
